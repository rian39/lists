---
title: "'Have you forgotten?' Conversion experiences in shopping lists"
author: "Adrian Mackenzie"
date: "`r Sys.Date()`"
output:
tufte::tufte_handout: default
citation_package: biber
latex_engine: xelatex
bibliography: /home/mackenza/ref_bibs/uni.bib
link-citations: yes
header-includes:
    - \usepackage{setspace}
    - \doublespacing
---

# 'Have you forgotten?' Conversion experiences in shopping lists

```{r setup, cache=FALSE, include=FALSE}
library(knitr)
library(tint)
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tint'))
options(htmltools.dir.version = FALSE)
output <- opts_knit$get("rmarkdown.pandoc.to")
opts_chunk$set(warning=FALSE)
opts_chunk$set(message=FALSE)
opts_chunk$set(error=TRUE)
opts_chunk$set(cache=TRUE)
```

## Abstract


This paper investigates practices of list-making associated with online grocery shopping at a  UK and global supermarket chain, Tesco. 
It reconstructs an account of how a recommender system suggests a small number of grocery items of personal relevance to each of the millions of online grocery shoppers. 
The recommender system  aim to up-sell and cross-sell some of the 250,000 items in the supermarket's inventory. 
I suggest that we can understand devices such as recommender systems as transformations or conversions of the temporal and agential fabric of the important social ordering device, the list.
While all written lists mix past and future actions, actual and potential events, their predictive re-writing in recommender systems adds a new operational dynamic.
The incompleteness of lists, their propensity to expand or change, becomes more important than their capacity to  enumerate or rank things. 
The conversion of lists from finite to in-definite in recommender systems suggests we might need to re-conceptualise how we think about social order and structure in media cultures.  

## 
 
Online grocery shopping at Tesco, a large supermarket chain based in the UK, includes recommendations for further grocery purchases under the title of 'Have you forgotten?'. When customers shop for groceries online, a list of five recommendations appear  at the checkout stage. These recommendations are the product of a recommender system, an important category of operational device in big data (see for instance [@Striphas_2015] for analysis of Netflix recommendation; [@Morris_2015]  or [@Seaver_2015] for an account of music recommendations]).  The question 'have you forgotten?' is followed by some  grocery items that could have been or are usually on a shopping list. The title of the suggestions is a bit misleading: the recommender system, as we will see, is not concerned with forgetting, with the many slips and oversights associated with shopping lists,  but rather with items that you didn't know what should be on your list, perhaps because you have never thought of putting them there.

The shelves of large contemporary supermarket are the endpoint of global logistic supply chains, in all their logistical, value-transforming and brand-mediated hypercomplexity (see [@Nielsen_TBA; @Tsing_2009] on logistics and power are the endpoint of global logistic supply chains, in all their logistical, value-transforming and brand-mediated hypercomplexity (see [@Nielsen_TBA; @Tsing_2009] on logistics and power)) Groceries display a geography of agriculture, industry, transport, communication and financialisation driven by flows of labour and capital. Encounters between this hyper-complex geography and people, even in the confines of a supermarket, are no simple matter, either for shoppers or for supermarket operators such as Tesco. While there are many facets to shopping [@Miller_2013], the shopping list, whether written on the back of an envelope, or saved as a list in an online grocery shopping system, is one intersectional ordering device. In a shopping trajectory (trawling along aisles packed with thousands of products, scrolling down screens or searching for particular brands amidst search results), lists inscribe some kind of order that filters or reduces the excessive propensities, claims, dazzle and distraction of commodities.

Hand-written shopping lists undoubtedly have ongoing importance and display interesting mixed ordering practices (see the list of handwritten shopping lists at [Grocery List](http://www.grocerylist.org)), online shopping lists take shape at the intersection of web and internet infrastructures, supply chain logistics, individualised practices and *habitus,* and increasingly, the predictive operations of  recommender systems. Just as the aisles and shelves of a supermarket present a commercially ordered geography of the world,  online shopping recommender systems generate lists that seek to align people to commodities they otherwise might have little relation to.  

```{r random-basket, echo=FALSE, cache=TRUE, fig.cap='Random basket'}
library(dplyr)
library(ggplot2)
library(readr)
library(knitr)
csvs = list.files('data', pattern = '*.csv',full.names=TRUE)
df = bind_rows(sapply(csvs, read_tsv))
kable(sample_n(df, size=5)$name, col.names='item')
```

The items on this list could be recommendations. They are only a few of them: five. The question: which five items and in what order?
```{r shopping-hand}
include_graphics('figure/1_shopping.png')
```

In terms of content, there are many similarities between shopping lists such as Figure \@ref(fig:shopping-hand)  -- a dozen lines scrawled on the back of a used envelope stuck on the refrigerator door  --  and the five ranked items shown to a customer on an online shopping site (as shown in Table \@ref(tab:random-basket)). While they are similar in content,  the list of five suggestions shown to Tesco's online customers at the checkout  exemplify a transformation in forms of ordering, a transformation that may affect the ontological texture of the world in the same way that commodification does. In its re-writing of the list, the recommender system effects a conversion experience in which individualised desire, consumption practices, value-chains, infrastructural time-space dynamics, the lives of others and the propensities of things all come into play.   ing a major contemporary form of ordering, the recommendation list, that we see in many  forms of news, media, entertainment, marketing, advertising. 

The presentation I saw was not really surprising or exceptional. It was a presentation by a data scientist to data scientists, describing an implementation of a predictive model as well as some of the problems and potentials of the predictive model.  I see the presentation itself as providing materials for an argument about our experience of predictive ordering. In this paper, I re-present the presentation using a mixture of theoretical and practical resources. The theory is largely recent media theory and its accounts of prediction in media. The practical resources are that of coding and predictive models.[^12]

[^12]: The problem of incomplete lists reminds me of the game 'I went shopping and I bought ... ' that adults sometimes play with children.  The game is about remembering a well-ordered list of items - 'a' is apple, 'b' is for banana, 'c' is for carrot.  The point of the game is to use the well-ordered set of labels provided by the sequence of letters in the alphabet to remember a random and perhaps very diverse list of items. The longer the game continues, the harder it is to remember since the shopping list grows longer. At the same time, each player repeats the list, and this  helps memorisation. One trick in playing this game is to choose items that are quite unexpected or increasingly random. Rather than `ham`, list `halva`; rather than `fish`, add `face cream.` This is especially useful for adults in beating children. The children have better memories for items they know, but adults know more unexpected items. On the other hand, very unexpected items are quite easy to remember and tend to even the score in the favour of the children. Balancing between the improbable and memorable, between a predictable list like 'apples, bananas, carrots' and an unusual list such as 'agave syrup, boccadilla, cilantro' is complicated. Something similar applies to recommender systems. The system can remember very many things. It may know everything you bought in the last few years on that site, and it can easily see what you would normally buy and add those as suggestions.

Despite being titled 'Have You Forgotten?' Tesco's recommender system does not suggest forgotten items.  It focuses on 'conversion events.' If a shopper does buy  recommended items, 'conversion' has occurred.  From a supermarket perspective, high-value conversion are desirable. There is little value in successfully recommending small or cheap items such as rubber bands or toothpicks.  From a data science perspective, conversion events, when they occur, confirm the power of predictive analytics to change the world in some way. From a sociological perspective, constructing and configuring recommender system in the name of conversion events, the recommender system effects an epistemic-operational transformation central to big data as an operational formation that responds to an ongoing control crisis [@Beniger_1986]. This central transformation is that the idea that data affords apprehension, visibility and management of associative propensities in the world.  Recommender systems, then, are concerned with the conversion of social order. 


## Lists in and as data 

Lists have long interested historians, anthropologists, sociologists, literary and media theorists. If, as the social anthropologist Jack Goody argued, lists are historically primary as forms of writing [@Goody_1986], and if list-making and its later variants (e.g. tables) precede discursive and narrative writing practices, then we might expect lists to function as powerful social ordering devices. As literary scholars suggest (see [@Mainberger_2003]), although lists have often been de-valued as literary forms, list-making often appears, even in literary form, whenever writing seeks to address, name, group or evoke totality, profusion, excess or abundance. From both  anthropological and literary perspectives, shopping lists have excellent reasons to exist: they are deeply rooted in urban social formations historically and culturally. Lists such as online grocery recommendations weave together people, infrastructures, things, places and predictions.[^1.456]    

[^1.456]: While lists intersect strongly with other meaning and sense-making practices in everyday life and popular culture, they can be refractory to discourse  and textual analysis methods built around models of language or speech, with its rules of syntax and grammar. On the one hand, lists are highly associative. They can be interpreted or decoded semiotically, although they exhibit  variations in textuality -- how they are written and read -- that thwart semiotic readings. On the other hand, as operational inscriptions, they can be treated ethnomethodologically, as the production of social order in a given setting. 

We might, in the light of their mutability, permeability and embedding in social order,   approach lists from various theoretical angles -- as asignifying semiotics as Maurizio Lazzarato calls it in his _Signs and Machines_ [@Lazzarato_2014] or as elements in 'a new order of spatio-temporal continuity for forms of economic, political and cultural life' as Celia Lury puts it in her account of topological turn [@Lury_2012]. In this paper, I focus on lists and list-making in terms of the predictive conditions of contemporary media experience [@Hansen_2015a].

All lists are forms of order. Mathematicians distinguish lists from sets on the basis of order. Formally, both lists and sets are collections, but a list also contains a mapping of its elements to an ordering, usually the natural or counting numbers. Sometimes this order is explicit. Numbers are written down one side of the list. Often the order is implicit to the spatial arrangement of the list.  Even random order can be  operationally important (for instance, in gambling machine or a psychology experiment, randomly order lists might be important).  Mappings between list order and social order are manifestly highly contingent. Listing practices in a given social setting, however, always index its engagement with necessary unstable dynamics of social order. From the perspective of social order, shopping lists, as they are co-constructed by a supermarket chain's recommender system, might be important intersectional zones for the conversion of the average everydayness of lists through data-analytic propensities. 


Mark Hansen has recently suggested, 

> predictive analytics are discoveries of micrological propensities that are not directly correlated with human understanding and affectivity and that do not by themselves cohere into clearly identifiable events [@Hansen_2015a, 111-112]

The question exercising much contemporary work on data, on media, platforms and finance pivots on the 'micrological propensities' that might be re-ordering lists. Hansen's argues that these propensities make probabilities into something real, some actually in things:  

>Whatever explanatory and causal value predictive analytics of large datasets have is, I suggest, ultimately rooted in this ontological transformation whereby probabilities are understood to be expressions of the actual propensity of things [@Hansen_2015a, 120] 

Hansen predicates prediction as an 'ontological transformation' transforming the relation between probabilities and things.  The practical and indeed empirical  question is whether such transformations in probability that bring 'the propensities of things'  can be detected and articulated in the prosaic setting of shopping lists and  shopping baskets. 'Conversion' might be a preferable term for these changes.  It highlights a range of re-orientations in subjects, experience, things, numbers and structures  entailed more than the subjective 'understood-to-be-expressed' formulation that Hansen emphasises.

## DunnHumby's work for Tesco

In one of the many industry-meets-academia events occurring in increasingly data science-oriented higher education institutions in the UK, speakers from industry, government and commerce described their work.[^11.1] Shreena Patel, a PhD graduate in statistics and operations research,  works as a data scientist for DunnHumby, a well-known [ customer science company ](https://www.dunnhumby.com/)  focusing on online grocery shopping at Tesco.  Speaking to an audience of statisticians and operations researchers, Patel   focused on the models underlying  shopping list recommendations. Her presentation was filled with graphs, numbers, and tables concerning different aspects of the 'Have you forgotten?' recommender system. Patel's presentation was not sociological. In order to explore the operational environment she described, and its predictive operations, I will reconstruct key features of her presentation using sample data drawn from the Tesco API ([Application Programmer Interface](https://devportal.tescolabs.com/docs/services/56c73b1bf205fd0ed81dbe7a/operations/56d5b2bee2813e0a0053c47c), [@Tesco.com_2017])  A sample of the most highly priced items in Tesco stores is shown in Table  \@ref(tab:expensive-groceries) and a range of data science, statistics and graphics libraries implemented in the widely-used statistical programming language `R` [@RDevelopmentCoreTeam_2010].[^11.2] 

[^11.1]: More than 100  Data Science Institutes have been set up in North America, Europe and UK since around 2012. The traffic between higher education and data analytics in industry is intense and flows in several forms: people funding, research findings, software and technical devices (code) and training. 

[^11.2]: 

```{r simulate-inventory, echo=FALSE, cache=TRUE, fig.margin=TRUE, fig.cap='Prices and quantities of things in the world'}
library(dplyr)
library(ggplot2)
library(readr)
library(knitr)

csvs = list.files('data', pattern = '*.csv',full.names=TRUE)
df = bind_rows(sapply(csvs, read_tsv))

g = ggplot(df, aes(x=price,y=ContentsQuantity)) + geom_point(alpha=0.3, size=0.9 ) + theme(axis.text=element_text(size=10))  + xlab('Price (£)')
g + ylab('Number of products') + scale_x_log10() + geom_smooth()
```

Figure \@ref(fig:simulate-inventory) plots something of the density of commodities in the Tesco grocery database. By virtue of their sheer abundance, the plot cannot easily separate the many items that Tesco sells, especially in the price range £1-£10.       

```{r  expensive-groceries, echo=FALSE, cache=TRUE, fig.margin=TRUE}
s = sample_n(df[df$price>=30,], size=10)
kable(s[order(s$price), c('name','price')])
```

I respond to Patel's presentation in two ways. First, by recounting, contextualising and commenting on some of the main steps in making the 'Have your forgotten?' list  as a example of the predictive sense-making done by data scientists and customer analytics teams working with transactional data in many commercial settings. Patel mentioned many of these steps only fleetingly in the presentation, for they are largely taken for granted as part of predictive analytic practice.  Second, by trying to step into the list-making practices  as an occasion to apprehend in the wider conversions they signal. As a sociological response to a data scientist presenting a model to an audience of statisticians and operations researchers, my interest lies less in specific technical innovations and more on how the ordering work done by recommender systems relates to problems of social order more generally.  The method of reconstruction borrows from the philosopher John Dewey a broad interest in how practical and abstract elements of experience are interwoven [@Dewey_1957]. More concretely, it draws heavily on the vision of 'interface methods' proposed by Noortje Marres [@Marres_2015] in order to keep in view both the highly technical orderings of lists in recommender system and the changes in social orderings they might imply or impel.

## How the shopping list was reconstituted in the 1990s

The main narrative of Patel's presentation concerned a shift from a well-established demographically-targeted data mining model for making recommendations to a predictive, probabilistic, personally relevant  re-writing of the shopping list in almost-realtime. The changes Patel described are increasingly widespread. While they are configured in Tesco-specific ways by DunnHumby (and this reflects a longer history), they are also broadly symptomatic. Tesco is the largest supermarket chain in UK, famous for its customer loyalty and targeted marketing programme known as 'Tesco  Clubcard,' which started in 1991. DunnHumby -- founded by operations researchers Edwina Dunn and Clive Humby -- is famous for having convinced the CEO of Tesco sometime in 1991 that a loyalty card program could change the supermarket chain's relationship to its customers.  Clive Humby's academic publications  are hard to track down, but an early paper given at the _Conference of Young Operational Researchers_ in Nottingham in 1984 suggests the direction that he, DunnHumby and later Tesco would take in constructing lists [@OKeefe_1984]. The abstract for Humby's presentation pre-figures an ongoing trajectory for data mining techniques aimed at eliciting detailed information on individual customer preferences.   

![Humby, 1984]( figure/2_humby_young_org_1994.png)

While Tesco  succeeded in data-mining its customers,  and perhaps became the UK's biggest supermarket with the help of data-mining in the 1990s, the online shopping environment  in 2016 is a bit different. It is no longer organised around campaigns involving special offers or redemption of points for demographically segmented customer (DunnHumby made heavy use of UK Census data). It can no longer rely on placement of goods in carefully chosen locations in stores. It needs or at least might want to continuously up-sell and cross-sell to customers who might hardly ever be in the supermarket itself. If 'Tesco is the clear winner in the online grocery market, in fact it takes almost 50p of every £1 spent on food shopping on the internet' [How SEO helps Tesco to dominate the online grocery market](https://econsultancy.com/blog/64841-how-seo-helps-tesco-to-dominate-the-online-grocery-market)[@Silverwood-Cope_2014], then Tesco itself has undergone some kind of conversion?

![People add to list](figure/online_lists_in_practice.png)

How could we characterise the shift from ClubCard data mining to online grocery markets?  Patel described her work at DunnHumby in terms of a shift in models from a 'rules-based list' to a 'relevancy model.' The relevancy model changed how the 'Have You Forgotten' list was constructed.   This shift is a typical of broader re-organisation of prediction in which existing predictive practices are being re-distributed and intensified in particular ways. The shift in models pivots on a more probabilistic structuring of lists.  The ostensible banality of the 'Have you forgotten?' recommendations betrays micrological signs of a topologically complex predictive infrastructure that subducts people and things in significant ways. 

## Reconfiguring recommendations

Academic researchers first began writing about recommender systems in the mid-1990s. They highlighted a potential shift from demographic-driven market research or data mining techniques to personalized recommendations. For instance, writing in 1997 in a special section of the _Communications of the ACM_ on recommender systems, Paul Resnick and Hal Varian (at that time Dean of Information Sciences at UC Berkeley, but currently Chief Economist at Google), made much of this difference [@Resnick_1997]. Resnick and Varian  emphasise the need to distinguish the emerging practices from data mining: 

> In everyday life, we rely on recommendations from other people. ... Recommender systems augment this natural social process. In a typical recommender  system, people provide recommendations as inputs, which the system then aggregates and directs to appropriate recipients. In some cases the primary transformation is in the aggregation; in others the system’s value lies in its ability to make good matches between the recommenders and those seeking recommendations.[@Resnick_1997,56] 

Writing just after the advent of web-based e-Commerce, Resnick and Varian  imagined recommender systems augmenting the 'collaborative filtering' that people do when they write and read reviews of products and services (for instance, on the travel website, TripAdvisor; [@Scott_2012]). In 1997, two decades ago, Resnick and Varian thought that 'people provide recommendations' and recommender systems would aggregate and rank for recipients. A slightly later review, [@Schafer_2001], diagrams an augmented 'natural social process' with a range of elements, technologies, inputs and outputs, with varying degrees of personalization. 

![Early e-commerce recommender system architecture](figure/schafer_recommendation_2001.png)


The shift from or conversion of  collaborative filtering appeared in Patel's presentations. Patel describes the move from 'a rules-based system'  to  a 'personal relevance model'. Each term in that phrase -- personal, relevance and model -- has some weight in the conversion work. We can assume that What Patel refers to as  the rule-based system refers  to the `apriori` algorithm, developed  by Rakesh Agrawal and Ramakrishnan Srikanti at IBM Research Alameda in the early 1990s [@Agrawal_1994]. A now-classic approach to 'market basket analysis,' it was listed as one of the top ten data mining algorithms in a survey conducted amongst data miners [@Wu_2008] and usually attracts a chapter in data-mining and some machine learning textbooks (e.g. [@Hastie_2009]). 


```{r arules, cache=TRUE, messages=FALSE, echo=FALSE, fig.margin=TRUE}
library(knitr)
library(arules)
library(arulesViz)
library(datasets)
data(Groceries)
itemFrequencyPlot(Groceries,topN=10,col=rainbow(10), ntype="absolute",  cex.axis = 0.55)
options(digits=2)
rules<- apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.15,minlen=2), appearance = list(default="rhs",lhs="whole milk"), control = list(verbose=F))
rules_sorted <- sort(rules, decreasing=TRUE,by="confidence")
kable(inspect(rules_sorted[1:5]))
```

The `apriori` algorithm finds sets of items that commonly occur together in transactions.  These commonly occurring sets are expressed as 'association rules.' For instance, using the `Groceries` dataset in the `R` package `arules,` we can readily see how Walmart *might* have discovered   the fabled association between diapers and beer.[^149.1] Figure \@ref(fig:arules) shows how often common items appear. `Whole milk` appears most frequently.  The `apriori` algorithm attempts to re-structure shopping lists using associations between items frequently bought together.  The association rule-based model for the `Groceries` dataset illustrates some characteristic ordering practices of 1990s-style data-mining approaches to lists (the `Groceries` dataset  was acquired from a 'local German supermarket' [@Hahsler_2006a]). The association between items is expressed in the form of _rules_ not a model. These rules, a term derived from older decision support literature, define sets of associations between items. As the product of a  counting practice, the association rules define sets of items or shopping baskets in terms of decreasing strength. Table \@ref(tab:arules) shows that the first rule has a stronger association than the second. Milk and vegetables is more likely an association than milk and buns.  A rules-based recommender system should, therefore, might put them in that order in the 'Have You Forgotten' list. 

[^149.1]: One of the founding narratives of big data customer analytics, the beer-diaper association was not, according to [@Power_2002] found through data mining at all.

While association rules and the `apriori` algorithm present social order in terms of sets of common associations weighted by frequency and hence implicitly by probability, an `apriori`-based recommender system contains no statistical model. It has no grasp on personal relevance since the associations it defines are only between things. Put in terms of the big data conversion, the rules-based system is not predictive. The `apriori` algorithm might be seen as an attempt to grasp the excess of things in a Tesco supermarket by identifying subsets whose closure derives from the associations between things that people construct (through buying habits, by virtue of a shopping list, etc.). In association rules, personhood or experience is irrelevant. The openness of these rules -- the `apriori` algorithm can product any number of rules of diminishing strength -- points towards the propensity of things, but cannot cannot individuate that propensity. 

Although data mining for association rules finds relations things, it struggles with the propensity of commodities to multiply. A simple illustration of the combinatorial problem faced by recommender system can be developed using the `Grocery` dataset.  If we take all the items in the `Grocery` dataset and paste them into the 'shopping list' box on the Tesco grocery website, each of the 169 items in the dataset yields dozens and sometimes thousands of products from Tesco online.

```{r tesco, echo=FALSE, cache=TRUE, fig.cap ='Tesco surplus',  fig.margin=TRUE}
    library(ggplot2)
    tesco = read.csv('data/reference_data/tesco_groceries.csv')
    total = sum(tesco$actual, na.rm=TRUE) 
    tesco = na.omit(tesco)
    tesco$actual = as.integer(tesco$actual)

    ggplot(tesco[tesco$actual>50,], aes(x=labels, y=actual, fill=labels)) +geom_bar(stat='identity') + coord_flip() + ggtitle('Tesco grocery items with more than 50 products') + theme(legend.position="none", axis.text=element_text(size=9))
```

Items in the`Groceries` dataset proliferate into a Tesco's list of branded variations. The 169 items of the `Groceries` dataset  expand into roughly `r total` Tesco items.  This is, I would suggest, a form of propensity in Hansen's terms, albeit one that occurs in, or very close to, the recommender system rather than  'in the world.' The multiple listed  in the `Grocery` dataset becomes more open in this setting through new impersonal intersections that must be bounded and ordered. The problem that the proliferation of things on the shelves of supermarket or grocery warehouse confronts is partly combinatorial. `r total` products (actually Patel mentioned 200,000 products) can be combined ways. If a typical shopping list has 20 items, then there are `r format(choose(total, 20), 2)` possible lists. Some of this vast number of possible shopping lists are highly unlikely in terms of typical purchasing patterns, but potentially highly interesting and value as recommendation. 

Amidst this proliferation of possibles multiple, the recommender system  starts to become a site of ordering -- in both senses of that term -- of things and people together in shopping lists. They are less the inscriptions of a discrete agent than an effect of an environment teeming with possible combinations and complementarities, a surplus of associations that the market needs to both maintain and limit. For instance, given that Tesco lists more than 1000 chicken products on its site, how can a customer practically discriminate between chicken products? Patel mentioned that some people log on and go straight to the checkout, allowing the recommender system to supply them with the suggestions that they then move to the shopping basket. A `r total` list of items is quite hard to list let alone read. Going straight to the recommendation list is the predictive equivalent of using an old shopping list and only adding new items when needed. 

## Ordering relevance through personhood

Combinatorial surplus, however, does not exhaust what might be found in the data. The relevancy model that DunnHumby has implemented for Tesco (as for so many other online businesses) seeks to balance the tensions between proliferating associations and relative closure. It responds to the problem of a world where the propensities of things and the propensities of people mingle with each other, yet threaten to disorder the list, the site of their mutual ordering.[^181.1] Big data discourse in its promissory mode attributes potency to personalization: 'most important, using big data we hope to identify specific individuals rather than groups' [@Mayer-Schönberger_2013]. The ontological transformation that media theorists such as Hansen suggests actually play out somewhat more conventionally: the associative propensities of things in the shopping list (as constructed by a recommender system) come together with forms of personhood associated with buying. A model of personhood based on purchase data delimits the propensities of things. 

[^181.1]: Rather than seeing predictive analytics as a closed form of an open world, I am suggesting that we might begin to locate it in the joint effectuations or intersections within models.

Patel described the data that Tesco keeps on grocery purchases: 'We have 52 weeks of purchase data for each customer.' These transformations have been widely discussed in the context of big data, where questions of who has how much data are usually most prominent (see [@Kitchin_2014]). In terms of the ways that lists order the world, the amount of data is not perhaps not so important as the way in which list-making is re-ordered via a model of personhood. 

It uses many shopping lists to identify associations between things and then uses the strongest associations to recommend additions to a current list. The 'relevancy model,' however, tries to do something different. 

Hansen attributes 'real potential' to data. In terms that somewhat echo big data epistemics, data has an associative propensity, according to Hansen:

> [S]urrounding any delimited predictive system is a larger field of data -- what I elsewhere call a “surplus of sensibility” -- that, viewed speculatively, indexes the causal efficacy of the total situation within which this delimited system operates. (Effectively, the latter gains its reliability from closing off this larger surplus of sensibility, thereby transforming an always excessive propensity into a [provisionally] closed dataset.) Because it affords data that exceed whatever any given predictive system might include, the data of the world’s causal efficacy -- the data constituting its real potentiality -- always and in principle facilitates knowledge that cannot be restricted to any particular agenda [@Hansen_2015, 126-7].

The underlying contrast between an open world and a 'provisionally closed dataset' is both enabling and limiting. Hansen's account opposes closed prediction to the open propensities of a data-world. A closed set is surrounded and conditioned by an open set teeming with potentials. If we think of the  new model that Patel describes as the delimited (or delimiting?) system within   the larger surplus of sensibility, how would that help us understand what the 'Have you forgotten?' recommendations do? It enables us to understand -- as we will see next -- what the Tesco relevancy model borrows from the world. It limits, on the other hand, our understanding of how predictive analytics, or any other ordering device, themselves remain open, require configuration, and only exist amidst technical ensembles whose operation remains obscure.   

```{r models}
include_graphics('figure/patel_compares_models.png')
```

Patel introduced the relevancy model with a graph familiar to machine learning and statistical modelling. (A sketch of her graph appears in Figure \@ref(fig:models).)  The graph plots the _precision_ -- the proportion of the recommended products that customers actually purchase -- for several different models. In media theoretical terms, the graph indexes the 'causal efficacy' of a recommender system, its capacity to include and transform propensities or 'real potentialities' into operational knowledge.   The graph compares the old rule-based recommender systems with some of the relevancy-model alternatives -- logistic regression, random forests, and a few others -- in terms of their predictions and how those predictions turned out. Patel dismissed most of the models quite quickly and focused only on one, the logistic regression model, which did better than other alternatives. Why did she say little about the model, which after all, produces the probabilistic predictions on which everything in the recommendation list depends and hence the conversion experience? Her interest lay in the precision of the recommendations, and, crucially for our purposes, in how 'most of the gain comes from thinking of good features.'

The models Patel so briefly mentioned are all classifiers, predictive models that spatially order  variables in a dataset as a high dimensional vector space. In the vector space, classification means drawing a decision boundary between points that henceforth belong to different classes of things [@Mackenzie_2015]. Here the classes of things are binary: `recommended` or `not recommended,` and for many of the models shown in the graph, membership of a class is expressed as a probability.  The logistic regression model generates probabilities for each product for each customer. Patel did not articulate the implication of the contrast between the left and right hand side of the graphic. Although the statistical framing appears explicitly (e.g. in measuring the precision of  recommender systems), the underlying shift to a statistical model is barely mentioned. In becoming predictive, the 'Have You Forgotten?' recommendations have included a version of the social in the form of a  model that seeks to identify relations between variables.  The relevance of recommendations pivots on the apprehension of social order by a model. 

## 7. Sparsity: output 20 recommendations from 200,000 products

Note that neither the infrastructure that marshals the Tesco customer data  - Patel will mention `Hadoop` -- nor the predictive analytic device -- the logistic regression model -- loom very large in the conversion of the recommender system. 'Most of the gain comes,' Patel observes, 'from thinking of good features.' While this is almost a truism in big data practice (see, for instance, [@Domingos_2012]), what counts as a good feature in a shopping list? 

A  very brief comment from Patel -- 'we have lots of zeros' -- points to the locus of this thinking: the vast, nearly empty matrix of customer-product associations. Given that most things in Tesco remain relatively inert in relation to each other, a matrix that records association between individual people and different products is bound to be mostly empty.   Say Tesco has 1 million online customers. Each of online  shopper has bought some selection of the 200,000 products. But the product-customer matrix, the basic vector-space in which all recommender systems operate, remain very sparse and unpopulated. The customer-product data matrix will be `r format(1e6 * 2e5)` in size. Given that any one customer is likely to only have bought 100 different products, the probability matrix will be close to `r format(100-(100/2e5)*100, 2)`% empty. The data is, as Patel put it, 'massively unbalanced', and such imbalance would heavily bias recommendation towards commonly and only impersonally relevant suggestions, suggestions that would likely not produce the desired conversion experience. Rather than a surplus of sensibility, predictive models, we might say, encounter their own dearth of sensibility in the data. 

```{r sparsity, echo=FALSE, cache=TRUE, fig.margin=FALSE, fig.cap ='Sparsity of the customer-product association matrix' }
library(ggplot2)
library(reshape2)
p = as.logical(rbinom(n=2e5, size=1, prob=0.001))
m = (matrix(p, nrow=1000, byrow=TRUE))

melted <- melt(m)
melted$value <- as.logical(melted$value)
ggplot(melted, aes(x = Var2, y = Var1, fill = value)) + geom_tile() +
    scale_fill_manual(values = c("grey", "black")) + xlab('Customer id') + ylab('Product id')
 
```

Since so many people buy milk, the recommendation system might end up always recommending milk products. So the data needs to be 'corrected' by, as Patel reports, removing -- under-sampling -- some of the data for common purchases.   Secondly, even if a recommendation system finds uncommon items that are good recommendations, they should neither be too cheap ('low spend'), nor, as we will see too similar to what the customer has already ordered.

HERE

In Hansen's or Badiou's terms, this is one way in which the dataset is closed. While 200,000 products is still a closed dataset since it has a defined number of items, undersampling alters the frequency of occurrence of very common items so that the predictive models are not distracted by them. 
 
So the raw data -- the 52 weeks of purchases for each customer -- can be rewoven with the database of Tesco products to increase the density of the data in ways that helps the model. Patel mentions some of the 'features' used in the new model but somewhat strangely at this point, her presentation turns toward the difficulties of measuring the success of the model and the infrastructural limitations on making recommendations in ways that are more alive to what is happening right now. 

## 8. Value: Balancing conversion vs spend
The value of an ordering
 
## 9. Metric: precision-at-K
 
The discussion of imperfection was brief - it incorporated the notion of recommendations weighted by price and the notion of precision (or the proportion of products actually purchased.). Patel simply said that this metric was imperfect, but without elaborating why. Instead she moved to describe some of the ways in which the recommender system worked in practice. 

## 10. Experimental settings: A/B allocation and uplift

![A/B testing](figure/a_b_tests.png) 

First of all, the new model was deployed experimentally in a random A/B controlled trial. This means that all customers are allocated to one of four categories as shown in the figure. For an identical shopping list, or even for the same person, it is possible to receive different recommendations.    

This randomising of the application of the model, drawing on protocols for randomised clinical trials first developed in the 1960s, is widely used in social media platforms and hence in all recommender systems. It is difficult to gauge how much. Facebook reports that it has more than 1 million models operation in its infrastructure. [TBA: ref for FBflow]   We could also discuss the integration of experiments into the operational infrastructures at Google.[REF tba] 
 
Random allocation of customers to the four categories (Test A, Test B, Control A, Control B) adds another layer of probability to the recommender system. This is done in the name of statistical validation of the effects or the uplift of the model. It also suggests that the effects of the model cannot be known in advance -- they can only be observed experimentally. Does this experimentality of the list point more to the propensities that Hansen says saturate experience?  

It is perhaps significant that this random allocation of customers to control and test groups occurs without any relation to the particular profile or propensities of the customer. It seeks to statistically validate the effects -- the *uplift* -- of the model on  conversion events by allowing the effects of the model on what people do to be measured. The lift or uplift refers to a change in events associated with the same groups of people. This effectively is an experiment in inhabiting two different worlds.  This sets up a feedback look between the predictive model, the logistic regression, and the operation of the recommendation system.[^122] Once the experimental uplift methodology is in place, the recommender system can constantly modify the recommendation list in light of subsequent data on the uplift. Predictions, then, are not bound by fixed parameters of the model. 

[^122]: See [@Lo_2002] for an introduction to the construction of experimental set ups for uplift models. Market researchers and data miners seem to have started adopting a statistically grounded experimental predictive practice sometimes around 2000. See for instance [@Almquist_2001]  Response modelling, propensity modelling and uplift modelling all seek to identify associations between 'treatments' or interventions and the 'Responders', the people  affected by the treatment.  Interestingly, these 


## 11. Infrastructure: realtime engine serves up top 5

![Realtime infrastructure and its limits](figure/hadoop.png)

While much discussion of recent media has highlighted its realtime operation, Patel's description of the infrastructure that implements the new model suggested a different temporal patterning. While raw data from Tesco Online feeds into the model every hour, it seems that recommendation list for each customer is only generated once a week. This makes sense. Customers only shop online every few days at the most, and in some cases, only every few weeks. To continually update the top 200 recommendations for every customer would be computationally expensive.   

Patel did not say much else about the infrastructure but the 200 recommendations generated each week are certainly never seen by the customers. The realtime engine that actually generates results shown on the Tesco Online website only shows 5 after 'applying business rules.' Again, this might be seen as corroborating Hansen and Badiou's insistence on the necessarily limited grasp of any prediction system on the situation it encounters. Even the new model, situated amidst its experimental setup cannot fully encompass the surplus of sensibility it encounters.    

The brief mention of specific infrastructural elements such as `hadoop` is, however, significant. It points to the temporality of infrastructures themselves as a component of list practices. The possibility of adjusting the recommendations for every customer weekly derives from an infrastructure capable of collecting data, assimilating that data to predictive model able to generate recommendations that are relevant to individuals amongst 200,000 products.  

## 12. Similarity, substitution, complementarity and micrological propensities

![Basket similarity](figure/patel_basket_similarity.png)

What does it matter that recommendations are relevant? Relevance is closely connected to propensities in the predictive condition. From DunnHumby's standpoint, the relevance of a recommendation relates directly to conversion. From the standpoint of the ontological conversion, and  if Hansen is right, the real or actual fabric of prediction resides in the propensities of things, what he calls the 'complex and diffuse calculus' that only subsequently and provisionally congeals as a conversion event. The 'micrological propensities' of things feed directly into an sense of the relevance of recommendations. 

How does the DunnHumby model deal with the micrological propensities of things? In her presentation, Patel  spent quite a lot of time addressing the problem of basket similarity in terms of substitutions and complementarities between things. She concludes by saying 'basket similarity is the future' and 'complementarities drive conversion.'  

Whereas the association rules focus on complementarities -- if you buy bread, you might want milk, if you buy beer, you might want diapers -- the substitutions look for micrological propensities that relate to gradations of preference, and variations in time and place.  Micrological propensities in the list relate to a particular time and place, amidst a continuously differentiable pathways between things.

How do things associate with each other in  substitutions and complementarities? Patel's comments on this are very brief but suggest that the 'Have You Forgotten?' list becomes an increasingly mutable entity, open to many substitutions and re-writings. Like the mention of `hadoop`, what Patel said about substitution and complementarity implies extensive but relatively well-established data practices. She speaks for instance of a 'design matrix of 14,000 columns' and the use of `L1 regularization` to drive coefficients down to zero. This is machine learning talk. What is being modelled here? No longer whether to recommend a product to a customer or not, but whether something could be substituted for what is already in the recommendation list.  

```{r biscuits, echo=FALSE, cache=TRUE, fig.margin=TRUE}
    library(readr)
    library(knitr)
    bisc = read_tsv('data/biscuits.csv')
    kable(bisc[order(bisc$price, decreasing=TRUE)[1:30], c('name','price')])
```

We can imagine this in terms of biscuits. A customer's recommendation list might already include `r bisc$name[10]`.  The 'self-learning substitution model' might be able to substitute something in that list that is of higher value, either because it is bigger or more expensive. 

 
## 13. Complementarity drives conversion 

![propensity_matrix](figure/propensity_conversions.png)

Similarly, to model the potential complementarities   between things entails a matrix of comparisons. `Nutella` complements `bananas,` but the converse is not necessarily true since `bananas` need not complement `nutella.`

How should we understand the effect of complementarity? Patel says that 'complementarity drives conversion.' That is, the matrix of similarities expressed as numerical associations between products best animates the conversion of recommendations into purchases.  

In the similarity matrix, shaped by the history of past recommendations and purchases made by this customer, lie the propensities of things that Hansen (following Popper) attributes to the world. Karl Popper speaks of the future as a 'crystallization of propensities' [@Popper_1990,18]   

## Conclusion

My investigation of lists has been centred around the transformation in their predictive underpinnings. How should we think about the list-making practices of these models, databases and computational infrastructures? Following Hansen, I have been suggesting that the predictive listing in the form of open-ended recommendation is symptomatic of changes in the distribution of media. The idea that media should no longer be considered as extensions or prostheses of human communication but the environmental situation in which different propensities play out is increasingly central to contemporary media theory. 

Hansen refers to an ontological transformation in which predictive media or recommender systems would be seen as the closed realisation of the actually open propensities of the world. By understanding the world in terms of propensities, he argues, we have a change to engage with contemporary predictive media. 

I have not explored the Whitehead-derived philosophy of propensities in the world that he develops there, only because I am not sure that we need speculative theory to grapple with the material practices of prediction. Already in the DunnHumby model and the Tesco 'Have You Forgotten?' recommendations, many of the traits of predictive processes are at work. 

I diverge from Hansen in how I account for this change. Rather than looking for a speculative theory that sets out a world in which prediction would be the closed form, 

- the **constitutive incompleteness** and openness of lists are very closely entangled with problems of social order under predictive conditions or amidst predictive operations. 
- shopping lists have complex temporal-material structure, since they mix prediction, infrastructural constraints, the individual characteristics of list-makers, statistical validations, database architectures and above all the **surplus sensibility** associated with  circulation of commodities.
- shopping lists undergoing a **conversion experience** as they become more probabilistic, but the idea that probabilities are the closed form of propensities does not account for the distribution of probabilities in relations between things, between people, between people and things, and between things and people.
- lists undergoing a 'conversion' experience as they become more probabilistic, but their heterogeneous character as operations caught up in models, infrastructures, experiments, and the flow of commodities means that they can never form a closed set or dataset. Amidst prediction they become constitutively incomplete. 

## References
